version: '3.8'

services:
  # Prometheus - ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì €ì¥ (OverlayFS ìµœì í™”)
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'     # ë°ì´í„° ë³´ì¡´ ê¸°ê°„ ë‹¨ì¶• (30dâ†’15d)
      - '--storage.tsdb.retention.size=2GB'     # ìŠ¤í† ë¦¬ì§€ í¬ê¸° ì œí•œ
      - '--storage.tsdb.wal-compression'        # WAL ì••ì¶• í™œì„±í™”
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--query.timeout=30s'                   # ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ ì„¤ì •
      - '--query.max-concurrency=10'            # ë™ì‹œ ì¿¼ë¦¬ ì œí•œ
    networks:
      - monitoring-network
      - app-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.4'           # CPU ì œí•œ ì™„í™” (0.5â†’0.4)
          memory: 384M          # ë©”ëª¨ë¦¬ ì œí•œ ì™„í™” (512Mâ†’384M)
        reservations:
          cpus: '0.15'          # CPU ì˜ˆì•½ ì™„í™” (0.2â†’0.15)
          memory: 192M          # ë©”ëª¨ë¦¬ ì˜ˆì•½ ì™„í™” (256Mâ†’192M)
    # OverlayFS ì´ìŠˆ ì™„í™”ë¥¼ ìœ„í•œ ë¡œê¹… ì œí•œ
    logging:
      driver: "json-file"
      options:
        max-size: "10m"       # ë¡œê·¸ íŒŒì¼ ìµœëŒ€ í¬ê¸°
        max-file: "3"         # ë¡œê·¸ íŒŒì¼ ìµœëŒ€ ê°œìˆ˜

  # Grafana - ë°ì´í„° ì‹œê°í™” ë° ëŒ€ì‹œë³´ë“œ (OverlayFS ìµœì í™”)
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3001:3000"  # ê¸°ì¡´ BFF ì„œë²„ì™€ í¬íŠ¸ ì¶©ëŒ ë°©ì§€
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_USERS_ALLOW_ORG_CREATE=false
      - GF_SECURITY_ALLOW_EMBEDDING=true
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_ALERTING_ENABLED=true
      - GF_EXPLORE_ENABLED=true
      - GF_FEATURE_TOGGLES_ENABLE=ngalert
      # OverlayFS ìµœì í™” ì„¤ì •
      - GF_LOG_LEVEL=warn                       # ë¡œê·¸ ë ˆë²¨ ìµœì í™” (infoâ†’warn)
      - GF_LOG_MODE=console
      - GF_ANALYTICS_REPORTING_ENABLED=false    # ë¶„ì„ ë³´ê³  ë¹„í™œì„±í™”
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false    # ì—…ë°ì´íŠ¸ í™•ì¸ ë¹„í™œì„±í™”
      - GF_DATABASE_WAL=true                    # WAL ëª¨ë“œ í™œì„±í™”
      - GF_DATABASE_CACHE_MODE=shared           # ìºì‹œ ëª¨ë“œ ìµœì í™”
      - GF_DASHBOARDS_MIN_REFRESH_INTERVAL=30s  # ìµœì†Œ ìƒˆë¡œê³ ì¹¨ ê°„ê²©
    networks:
      - monitoring-network
    restart: unless-stopped
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          cpus: '0.25'          # CPU ì œí•œ ì™„í™” (0.3â†’0.25)
          memory: 192M          # ë©”ëª¨ë¦¬ ì œí•œ ì™„í™” (256Mâ†’192M)
        reservations:
          cpus: '0.08'          # CPU ì˜ˆì•½ ì™„í™” (0.1â†’0.08)
          memory: 96M           # ë©”ëª¨ë¦¬ ì˜ˆì•½ ì™„í™” (128Mâ†’96M)
    # OverlayFS ì´ìŠˆ ì™„í™”ë¥¼ ìœ„í•œ ë¡œê¹… ì œí•œ
    logging:
      driver: "json-file"
      options:
        max-size: "5m"        # ë¡œê·¸ íŒŒì¼ ìµœëŒ€ í¬ê¸°
        max-file: "2"         # ë¡œê·¸ íŒŒì¼ ìµœëŒ€ ê°œìˆ˜

  # Node Exporter - ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - monitoring-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 64M
        reservations:
          cpus: '0.05'
          memory: 32M

  # Redis Exporter - Redis ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ì£¼ì„ ì²˜ë¦¬ - ë³„ë„ ì‹¤í–‰ ì‹œ Redis ì—†ìŒ)
  # redis-exporter:
  #   image: oliver006/redis_exporter:latest
  #   container_name: redis-exporter
  #   env_file:
  #     - ./monitoring/.env
  #   ports:
  #     - "9121:9121"
  #   environment:
  #     - REDIS_ADDR=redis://redis:6379
  #     - REDIS_PASSWORD=${REDIS_PASSWORD}
  #   networks:
  #     - monitoring-network
  #     - app-network
  #   restart: unless-stopped
  #   depends_on:
  #     - redis
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.1'
  #         memory: 64M
  #       reservations:
  #         cpus: '0.05'
  #         memory: 32M

  # Member DB Exporter - íšŒì› ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
  # member-db-exporter:
  #   image: prometheuscommunity/postgres-exporter:latest
  #   container_name: member-db-exporter
  #   ports:
  #     - "9187:9187"
  #   environment:
  #     - DATA_SOURCE_NAME=postgresql://postgres:1234@practice_app-network:5432/fastify_member_db?sslmode=disable
  #   networks:
  #     - monitoring-network
  #     - app-network
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.1'
  #         memory: 64M
  #       reservations:
  #         cpus: '0.05'
  #         memory: 32M

  # # Product DB Exporter - ìƒí’ˆ ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
  # product-db-exporter:
  #   image: prometheuscommunity/postgres-exporter:latest
  #   container_name: product-db-exporter
  #   ports:
  #     - "9188:9187"
  #   environment:
  #     - DATA_SOURCE_NAME=postgresql://product_db:product_bongsoo@practice_app-network:5434/product_db?sslmode=disable
  #   networks:
  #     - monitoring-network
  #     - app-network
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.1'
  #         memory: 64M
  #       reservations:
  #         cpus: '0.05'
  #         memory: 32M

  # cAdvisor - ì»¨í…Œì´ë„ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (OverlayFS ìµœì í™”)
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    privileged: true
    command:
      - '--docker_only=true'                    # Dockerë§Œ ëª¨ë‹ˆí„°ë§
      - '--housekeeping_interval=60s'           # ìˆ˜ì§‘ ê°„ê²© ìµœì í™” (30sâ†’60s)
      - '--max_housekeeping_interval=120s'      # ìµœëŒ€ ê°„ê²© (35sâ†’120s)
      - '--store_container_labels=false'        # ë ˆì´ë¸” ì €ì¥ ë¹„í™œì„±í™”
      - '--disable_metrics=accelerator,cpu_topology,disk,memory_numa,tcp,udp,percpu,sched,process,hugetlb,referenced_memory,resctrl,cpuset,advtcp,memory_numa'  # ë¶ˆí•„ìš”í•œ ë©”íŠ¸ë¦­ ë¹„í™œì„±í™”
      - '--event_storage_event_limit=default'   # ì´ë²¤íŠ¸ ì œí•œ
      - '--event_storage_age_limit=default'     # ì´ë²¤íŠ¸ ë³´ì¡´ ê¸°ê°„
    networks:
      - monitoring-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.15'          # CPU ì œí•œ ì™„í™” (0.2â†’0.15)
          memory: 96M           # ë©”ëª¨ë¦¬ ì œí•œ ì™„í™” (128Mâ†’96M)
        reservations:
          cpus: '0.08'          # CPU ì˜ˆì•½ ì™„í™” (0.1â†’0.08)
          memory: 48M           # ë©”ëª¨ë¦¬ ì˜ˆì•½ ì™„í™” (64Mâ†’48M)
    # OverlayFS ì´ìŠˆ ì™„í™”ë¥¼ ìœ„í•œ ë¡œê¹… ì œí•œ
    logging:
      driver: "json-file"
      options:
        max-size: "5m"        # ë¡œê·¸ íŒŒì¼ ìµœëŒ€ í¬ê¸°
        max-file: "2"         # ë¡œê·¸ íŒŒì¼ ìµœëŒ€ ê°œìˆ˜

  # Config Preprocessor - í™˜ê²½ë³€ìˆ˜ ì¹˜í™˜
  alertmanager-config:
    image: alpine:latest
    container_name: alertmanager-config
    env_file:
      - ./monitoring/.env
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/tmp/alertmanager.yml.template:ro
      - alertmanager_config:/etc/alertmanager
    command: >
      /bin/sh -c "
        apk add --no-cache gettext &&
        envsubst < /tmp/alertmanager.yml.template > /etc/alertmanager/alertmanager.yml &&
        echo 'Configuration processed successfully'
      "
    restart: "no"

  # AlertManager - ì•Œë¦¼ ê´€ë¦¬
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    depends_on:
      - alertmanager-config
    ports:
      - "9093:9093"
    volumes:
      - alertmanager_config:/etc/alertmanager:ro
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
      - '--cluster.advertise-address=0.0.0.0:9093'
    networks:
      - monitoring-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 64M
        reservations:
          cpus: '0.05'
          memory: 32M

  # ìë™ í—¬ìŠ¤ì²´í¬ ì„œë¹„ìŠ¤
  health-monitor:
    image: alpine:latest
    container_name: health-monitor
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./monitoring:/monitoring:ro
    environment:
      - HEALTH_CHECK_INTERVAL=300  # 5ë¶„ë§ˆë‹¤ ì²´í¬
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL:-}
      - EMAIL_ALERTS=${EMAIL_ALERTS:-false}
    command: |
      sh -c '
        apk add --no-cache curl jq
        echo "Starting automated health monitoring..."
        
        # í—¬ìŠ¤ì²´í¬ í•¨ìˆ˜
        check_service() {
          local service=$$1
          local port=$$2
          local endpoint=$$3
          local timeout=5
          
          if curl -s --connect-timeout $$timeout "http://localhost:$$port$$endpoint" > /dev/null 2>&1; then
            return 0
          else
            return 1
          fi
        }
        
        # ì•Œë¦¼ í•¨ìˆ˜ (Slack)
        send_slack_alert() {
          local message="$$1"
          if [ ! -z "$$SLACK_WEBHOOK_URL" ]; then
            curl -X POST -H "Content-type: application/json" \
              --data "{\"text\":\"ğŸš¨ Docker Monitoring Alert: $$message\"}" \
              "$$SLACK_WEBHOOK_URL" 2>/dev/null || true
          fi
        }
        
        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì²´í¬
        check_resources() {
          # ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ì²´í¬ (85% ì´ìƒì‹œ ê²½ê³ )
          local disk_usage=$$(df -h / | awk "NR==2 {print $$5}" | sed "s/%//")
          if [ $$disk_usage -gt 85 ]; then
            echo "âš  High disk usage: $$disk_usage%"
            send_slack_alert "High disk usage: $$disk_usage%"
          fi
        }
        
        # ë©”ì¸ í—¬ìŠ¤ì²´í¬ ë£¨í”„
        while true; do
          echo "Running health check at $$(date)"
          failed_services=""
          
          # ê° ì„œë¹„ìŠ¤ ì²´í¬
          services="prometheus:9090:/api/v1/query?query=up grafana:3001:/api/health cadvisor:8080:/healthz node-exporter:9100:/metrics"
          
          for service_info in $$services; do
            service_name=$$(echo $$service_info | cut -d: -f1)
            port=$$(echo $$service_info | cut -d: -f2)
            endpoint=$$(echo $$service_info | cut -d: -f3-)
            
            if ! check_service $$service_name $$port "/$$endpoint"; then
              failed_services="$$failed_services $$service_name"
              echo "âŒ $$service_name failed health check"
            else
              echo "âœ… $$service_name healthy"
            fi
          done
          
          # ì»¨í…Œì´ë„ˆ ìƒíƒœ ì²´í¬
          containers="prometheus grafana cadvisor node-exporter"
          stopped_containers=""
          
          for container in $$containers; do
            if ! docker ps --format "{{.Names}}" | grep -q "^$$container$$"; then
              stopped_containers="$$stopped_containers $$container"
            fi
          done
          
          # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì²´í¬
          check_resources
          
          # ì•Œë¦¼ ì „ì†¡
          if [ ! -z "$$failed_services" ] || [ ! -z "$$stopped_containers" ]; then
            alert_message="Health check failed!"
            [ ! -z "$$failed_services" ] && alert_message="$$alert_message Services down:$$failed_services"
            [ ! -z "$$stopped_containers" ] && alert_message="$$alert_message Containers stopped:$$stopped_containers"
            
            send_slack_alert "$$alert_message"
            echo "ğŸš¨ Alert sent: $$alert_message"
          fi
          
          echo "Health check completed. Next check in $$HEALTH_CHECK_INTERVAL seconds."
          sleep $$HEALTH_CHECK_INTERVAL
        done
      '
    restart: unless-stopped
    networks:
      - monitoring-network
    deploy:
      resources:
        limits:
          cpus: '0.05'
          memory: 64M
        reservations:
          cpus: '0.02'
          memory: 32M
    logging:
      driver: "json-file"
      options:
        max-size: "2m"
        max-file: "2"
    profiles:
      - health-check

volumes:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  alertmanager_data:
    driver: local
  alertmanager_config:
    driver: local

networks:
  monitoring-network:
    driver: bridge
  app-network:
    driver: bridge
    name: practice_app-network


  # Docker ì‹œìŠ¤í…œ ì •ë¦¬ ì„œë¹„ìŠ¤ (OverlayFS ìµœì í™”)
  docker-cleanup:
    image: alpine:latest
    container_name: docker-cleanup
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: |
      sh -c '
        echo "Starting Docker cleanup service..."
        while true; do
          echo "Running cleanup at $$(date)"
          
          # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ ì •ë¦¬ (24ì‹œê°„ ì´ì „)
          docker image prune -af --filter "until=24h" 2>/dev/null || true
          
          # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì»¨í…Œì´ë„ˆ ì •ë¦¬
          docker container prune -f --filter "until=24h" 2>/dev/null || true
          
          # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë„¤íŠ¸ì›Œí¬ ì •ë¦¬
          docker network prune -f 2>/dev/null || true
          
          # ë¹Œë“œ ìºì‹œ ì •ë¦¬ (1GB ì´ìƒì¼ ë•Œ)
          CACHE_SIZE=$$(docker system df --format "table {{.Type}}\t{{.Size}}" | grep "Build Cache" | awk "{print \$3}" | sed "s/GB//" | cut -d"." -f1)
          if [ "$$CACHE_SIZE" -gt 1 ] 2>/dev/null; then
            docker builder prune -af --filter "until=24h" 2>/dev/null || true
          fi
          
          echo "Cleanup completed. Next run in 6 hours."
          sleep 21600  # 6ì‹œê°„ ëŒ€ê¸°
        done
      '
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.05'
          memory: 32M
        reservations:
          cpus: '0.02'
          memory: 16M
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
    profiles:
      - cleanup

# ì‹¤í–‰ ëª…ë ¹ì–´:
# docker-compose -f docker-compose.yaml -f docker-compose.min.yml -f docker-compose.monitoring.yml up --build -d
# 
# OverlayFS ì´ìŠˆ ì™„í™”ë¥¼ ìœ„í•œ ì •ë¦¬ ì„œë¹„ìŠ¤ í¬í•¨í•˜ì—¬ ì‹¤í–‰:
# docker-compose -f docker-compose.yaml -f docker-compose.min.yml -f docker-compose.monitoring.yml --profile cleanup up --build -d
